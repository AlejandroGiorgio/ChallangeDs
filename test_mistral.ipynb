{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from typing import List, Dict, Callable\n",
    "from src.rag import RAGEmbeddingEngine\n",
    "from pydantic import BaseModel, ValidationError\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response  = ollama.list()\n",
    "rag_tool = RAGEmbeddingEngine(index_path=\"embeddings/got_index.faiss\", metadata_path=\"embeddings/got_metadata.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_tool.search(\"Viserys corona oro fundido\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(query: str) -> List[Dict[str, str]]:\n",
    "    response = rag_tool.search(query, k = 5)\n",
    "    formatted_response = []\n",
    "    for chunk in response:\n",
    "        retrieved_text = chunk[1]\n",
    "        string_to_add = f\"-Pagina {retrieved_text.page_number} - '{retrieved_text.text}'\"\n",
    "        formatted_response.append(string_to_add)\n",
    "    return formatted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_functions: Dict[str, Callable] = {\n",
    "    \"get_embeddings\": get_embeddings\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = (\"Eres un experto en Juego de Tronos pero nunca respondes con informacion que no se encuentre en los fragmentos de texto embebidos.\"\n",
    "                          \"Utilizas fragmentos de texto embebidos en el libro para responder las preguntas.\"\n",
    "                          \"Se te hará una pregunta sobre el libro y deberás responderla de la forma más precisa y clara.\"\n",
    "                          \"Si no sabes la respuesta, puedes decir que no cuentas con esa informacion y pedir mas datos para contestar adecuadamente.\"\n",
    "                          \"Siempre antes de responder, consultar la funcion 'get_embeddings' para buscar fragmentos de texto embebidos en el libro que puedan ayudarte a responder la pregunta.\"\n",
    "                          \"Para utilizar la funcion 'get_embeddings', debes hacer una query con el termino que deseas buscar en el libro.\"\n",
    "                          \"Responde unicamente con la informacion que encuentres en los fragmentos de texto embebidos.\"\n",
    "                          \"Nunca respondas con informacion que no se encuentre en los fragmentos de texto embebidos.\"\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ollama.chat(\n",
    "    model=\"mistral:latest\",\n",
    "    messages=[\n",
    "        {   \"role\": \"system\", \"content\": system_prompt_template,\n",
    "            \"role\": \"user\", \"content\": \"¿Quien es Ned Stark?\"},\n",
    "    ],\n",
    "    tools=[get_embeddings]\n",
    ")\n",
    "\n",
    "if res.message.tool_calls:\n",
    "    for tool_call in res.message.tool_calls:\n",
    "        tool_name = tool_call.function.name\n",
    "        tool_args = tool_call.function.arguments\n",
    "        tool_function = available_functions.get(tool_name)\n",
    "        logging.info(f\"Running tool {tool_name} with args {tool_args}\")\n",
    "        if tool_function:\n",
    "            tool_response = tool_function(**tool_args)\n",
    "        else:\n",
    "            tool_response = (f\"Function {tool_name} not found in available functions\")\n",
    "\n",
    "    tool_response    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.expert_agent import ExpertAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = ExpertAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = expert.chat(\"Se dice cuanto media?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert.chat_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
